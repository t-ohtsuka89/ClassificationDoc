seed: 123

log_path: config/RoBERTa/summary/level1-v1.log
save_dir: ./models/checkpoints/RoBERTa/level1-v1
save_top_k: 1
prefix: RoBERTa

dataset:
  data_module: TransformersDataModule
  text_dir: data/summary
  label_dir: data/label_level1
  batch_size: 32

method: Transformers
tuning_lr: false

trainer:
  gpus:
    - 1
  max_epochs: -1
  min_epochs: 1000

model:
  model_name: nlp-waseda/roberta-base-japanese
  optimizer: Ranger
  learning_rate: 1.0e-3
  enable_lldr: False
  mixout: 0.5
  n_warmup: 100
  num_train_steps: 1000

early_stopping:
  monitor: train_loss
  mode: min
  patience: 100
