seed: 123

log:
  filename: train.log

dataset:
  text_dir: data/texts
  label_dir: data/label_level2
  batch_size: 32

method: Transformers
tuning_lr: true

trainer:
  max_epochs: -1
  min_epochs: 1000
  accumulate_grad_batches: 8

model:
  model_name: nlp-waseda/roberta-base-japanese
  optimizer: Ranger
  learning_rate: 1.0e-3
  enable_lldr: False

early_stopping:
  monitor: train_loss
  mode: min
  patience: 100
