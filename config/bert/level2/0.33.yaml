seed: 123

log_path: config/bert/level2/v1.log
save_dir: ./models/checkpoints/bert/level2-v1
save_top_k: 1
prefix: bert

dataset:
  data_module: TransformersDataModule
  text_dir: data/texts
  label_dir: data/label_level2

method: Bert

train:
  batch_size: 32
  max_epochs: 10000
  min_epochs: 1000
  accumulate_grad_batches: 4

model:
  model_name: cl-tohoku/bert-base-japanese
  optimizer: Ranger
  learning_rate: 1.0e-3
  T_max: null

early_stopping:
  monitor: train_loss
  mode: min
  patience: 20
